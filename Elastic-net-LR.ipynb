{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e129de1",
   "metadata": {},
   "source": [
    "Elastic-net Logistic Regression validation on an external cohort (NO fixed weights).\n",
    "\n",
    "Goal:\n",
    "- Evaluate whether a *gene set* (e.g., your 37 genes) can classify AD vs Control\n",
    "  in an external dataset, using nested cross-validation (hyperparameter tuning inside CV).\n",
    "\n",
    "What it does:\n",
    "1) Loads expression gene×sample CSV + metadata CSV\n",
    "2) Builds samples×genes matrix (HGNC symbol rows collapsed by mean)\n",
    "3) Keeps only genes in GENE_LIST that exist in the dataset\n",
    "4) Runs nested CV:\n",
    "   - Outer CV: performance estimation (RepeatedStratifiedKFold)\n",
    "   - Inner CV: tune elastic-net (C, l1_ratio) using GridSearchCV\n",
    "5) Produces pooled out-of-fold probabilities (one per sample)\n",
    "6) Reports:\n",
    "   - Fold metrics (mean ± SD) at THRESHOLD\n",
    "   - Pooled OOF metrics at THRESHOLD\n",
    "   - Optional: pooled OOF metrics at \"best threshold\" (max balanced accuracy)\n",
    "   - Bootstrap 95% CI on pooled OOF predictions\n",
    "7) Saves results/oof_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b99f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26a0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_CSV = Path(r\"data/GSE125583/DE_data/GSE125583_log2cpm_SELECTED_GENES.csv\")\n",
    "META_CSV = Path(r\"data/GSE125583/DE_data/metadata_200samples.csv\")\n",
    "\n",
    "SAMPLE_COL = \"geo_accession\"\n",
    "LABEL_COL = \"diagnosis:ch1\"\n",
    "POSITIVE_LABEL = \"Alzheimer's disease\"\n",
    "\n",
    "# If known, set it; else set None to use last column\n",
    "GENE_COL_NAME: Optional[str] = \"Gene\"\n",
    "\n",
    "# Paste your 37 genes here (HGNC symbols)\n",
    "GENE_LIST: List[str] = [\n",
    "    \"ADAM33\", \"AEBP1\", \"CCDC102A\",\"CLDN9\", \"GFAP\",\"HSPB1\",\"HSPB7\",\"KANK2\", \"KLF15\", \"MRGPRF\", \"NUPR1\", \"PIK3R5\", \"PRELP\", \"PRX\", \"TCEA3\", \"TMPRSS5\", \"CHML\", \"ELOVL4\",\n",
    "    \"GAD1\", \"GAD2\", \"HPRT1\", \"ITFG1\", \"MAS1\", \"NAP1L5\", \"NCALD\", \"NEUROD6\", \"NRN1\", \"OPN3\", \"RAB3B\", \"RAB3C\", \"RGS4\", \"RPH3A\", \"SCG2\", \"SERPINI1\", \"STAT4\", \"TRIM36\"\n",
    "]\n",
    "\n",
    "# CV / evaluation\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 20\n",
    "INNER_SPLITS = 5\n",
    "SEED = 42\n",
    "THRESHOLD = 0.50\n",
    "BOOTSTRAP = 1000\n",
    "\n",
    "# Elastic-net tuning grid\n",
    "C_GRID = np.logspace(-3, 3, 13)\n",
    "L1_RATIO_GRID = np.linspace(0.0, 1.0, 11)\n",
    "\n",
    "# Outputs\n",
    "OUTDIR = Path(\"results\")\n",
    "PREDICTIONS_OUT = OUTDIR / \"oof_predictions.csv\"\n",
    "BESTPARAMS_OUT = OUTDIR / \"best_params_per_fold.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45bb2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Metrics:\n",
    "    auc: float\n",
    "    auprc: float\n",
    "    balanced_accuracy: float\n",
    "    sensitivity: float\n",
    "    specificity: float\n",
    "    mcc: float\n",
    "    brier: float\n",
    "\n",
    "\n",
    "def expected_calibration_error(y_true: np.ndarray, y_prob: np.ndarray, bins: int = 10) -> float:\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, y_prob, n_bins=bins, strategy=\"quantile\")\n",
    "    if len(frac_pos) == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.mean(np.abs(frac_pos - mean_pred)))\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, y_prob: np.ndarray, threshold: float) -> Metrics:\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else float(\"nan\")\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "    return Metrics(\n",
    "        auc=float(roc_auc_score(y_true, y_prob)),\n",
    "        auprc=float(average_precision_score(y_true, y_prob)),\n",
    "        balanced_accuracy=float(balanced_accuracy_score(y_true, y_pred)),\n",
    "        sensitivity=float(sens),\n",
    "        specificity=float(spec),\n",
    "        mcc=float(matthews_corrcoef(y_true, y_pred)),\n",
    "        brier=float(brier_score_loss(y_true, y_prob)),\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize(name: str, values: List[float]) -> str:\n",
    "    arr = np.array(values, dtype=float)\n",
    "    return f\"{name}: {np.nanmean(arr):.3f} ± {np.nanstd(arr):.3f}\"\n",
    "\n",
    "\n",
    "def bootstrap_ci(\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    threshold: float,\n",
    "    n_bootstrap: int,\n",
    "    seed: int,\n",
    ") -> Dict[str, Tuple[float, float]]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(y_true)\n",
    "\n",
    "    keys = [\"auc\", \"auprc\", \"balanced_accuracy\", \"sensitivity\", \"specificity\", \"mcc\", \"brier\"]\n",
    "    stats = {k: [] for k in keys}\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        yt = y_true[idx]\n",
    "        yp = y_prob[idx]\n",
    "        if len(np.unique(yt)) < 2:\n",
    "            continue\n",
    "\n",
    "        m = metrics_at_threshold(yt, yp, threshold)\n",
    "        stats[\"auc\"].append(m.auc)\n",
    "        stats[\"auprc\"].append(m.auprc)\n",
    "        stats[\"balanced_accuracy\"].append(m.balanced_accuracy)\n",
    "        stats[\"sensitivity\"].append(m.sensitivity)\n",
    "        stats[\"specificity\"].append(m.specificity)\n",
    "        stats[\"mcc\"].append(m.mcc)\n",
    "        stats[\"brier\"].append(m.brier)\n",
    "\n",
    "    ci = {}\n",
    "    for k in keys:\n",
    "        vals = stats[k]\n",
    "        if len(vals) == 0:\n",
    "            ci[k] = (float(\"nan\"), float(\"nan\"))\n",
    "        else:\n",
    "            lo, hi = np.percentile(vals, [2.5, 97.5])\n",
    "            ci[k] = (float(lo), float(hi))\n",
    "    return ci\n",
    "\n",
    "\n",
    "def choose_threshold_max_balanced_accuracy(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "\n",
    "    # remove non-finite\n",
    "    finite = np.isfinite(thr)\n",
    "    fpr, tpr, thr = fpr[finite], tpr[finite], thr[finite]\n",
    "\n",
    "    # avoid degenerate ~0 thresholds (common when scores are extremely close)\n",
    "    keep = thr > 1e-6\n",
    "    if keep.sum() == 0:\n",
    "        keep = thr > 0\n",
    "    if keep.sum() == 0:\n",
    "        keep = np.ones_like(thr, dtype=bool)\n",
    "\n",
    "    fpr2, tpr2, thr2 = fpr[keep], tpr[keep], thr[keep]\n",
    "    bal_acc = (tpr2 + (1 - fpr2)) / 2.0\n",
    "    best_idx = int(np.argmax(bal_acc))\n",
    "    return float(thr2[best_idx])\n",
    "\n",
    "\n",
    "def build_samples_x_genes(expr_csv: Path, meta_csv: Path) -> pd.DataFrame:\n",
    "    expr = pd.read_csv(expr_csv)\n",
    "    meta = pd.read_csv(meta_csv)\n",
    "\n",
    "    gene_col = GENE_COL_NAME if GENE_COL_NAME is not None else expr.columns[-1]\n",
    "    if gene_col not in expr.columns:\n",
    "        raise ValueError(f\"GENE_COL_NAME='{GENE_COL_NAME}' not found in expression CSV columns.\")\n",
    "    if SAMPLE_COL not in meta.columns:\n",
    "        raise ValueError(f\"SAMPLE_COL='{SAMPLE_COL}' not found in metadata CSV columns.\")\n",
    "    if LABEL_COL not in meta.columns:\n",
    "        raise ValueError(f\"LABEL_COL='{LABEL_COL}' not found in metadata CSV columns.\")\n",
    "\n",
    "    # sample columns = everything except gene_col and (likely) an ID column\n",
    "    exclude = {gene_col}\n",
    "    first_col = expr.columns[0]\n",
    "    if first_col != gene_col and (\n",
    "        first_col.lower() in {\"id\", \"index\", \"ensembl\", \"ensg\"} or expr[first_col].dtype == object\n",
    "    ):\n",
    "        exclude.add(first_col)\n",
    "\n",
    "    sample_cols = [c for c in expr.columns if c not in exclude]\n",
    "    gx = expr[[gene_col] + sample_cols].dropna(subset=[gene_col]).copy()\n",
    "    gx[gene_col] = gx[gene_col].astype(str)\n",
    "\n",
    "    # collapse duplicated symbols\n",
    "    gx = gx.groupby(gene_col, as_index=True)[sample_cols].mean()\n",
    "\n",
    "    # transpose -> samples × genes\n",
    "    sxg = gx.T\n",
    "    sxg.index.name = SAMPLE_COL\n",
    "    sxg = sxg.reset_index()\n",
    "\n",
    "    df = meta[[SAMPLE_COL, LABEL_COL]].merge(sxg, on=SAMPLE_COL, how=\"inner\")\n",
    "    if df.empty:\n",
    "        raise ValueError(\n",
    "            \"After merging metadata and expression, got 0 rows. \"\n",
    "            \"Check that metadata sample IDs match the expression sample column names.\"\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00fa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    if len(GENE_LIST) == 0:\n",
    "        raise ValueError(\"GENE_LIST is empty. Paste your 37 genes into GENE_LIST first.\")\n",
    "\n",
    "    OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = build_samples_x_genes(EXPR_CSV, META_CSV)\n",
    "\n",
    "    y = (df[LABEL_COL].astype(str) == str(POSITIVE_LABEL)).astype(int).to_numpy()\n",
    "    if y.sum() == 0 or y.sum() == len(y):\n",
    "        raise ValueError(\"Both classes are required (need positives and negatives).\")\n",
    "\n",
    "    # Use only the provided gene set\n",
    "    genes = [g for g in GENE_LIST if g in df.columns]\n",
    "    missing = sorted(set(GENE_LIST) - set(genes))\n",
    "\n",
    "    print(f\"Samples after merge: {len(df)} | AD: {int(y.sum())} | Control: {int(len(y) - y.sum())}\")\n",
    "    print(f\"Genes requested: {len(GENE_LIST)} | present: {len(genes)} | missing: {len(missing)}\")\n",
    "    if missing:\n",
    "        print(\"Missing genes (first 15):\", missing[:15])\n",
    "\n",
    "    if len(genes) < 5:\n",
    "        raise ValueError(\"Too few genes present from GENE_LIST after intersection. Check gene symbols/case.\")\n",
    "\n",
    "    X = df.loc[:, genes].copy()\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    if X.isna().any().any():\n",
    "        raise ValueError(\"Expression matrix contains non-numeric values after coercion. Fix input CSV.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # NO NESTED CV, NO OOF:\n",
    "    # Train/Val/Test holdout + tune on train only, choose threshold on val, evaluate once on test\n",
    "    # ----------------------------\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "        X, y, test_size=0.30, stratify=y, random_state=SEED\n",
    "    )\n",
    "    X_val, X_te, y_val, y_te = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=0.50, stratify=y_tmp, random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Model + tuning\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            solver=\"saga\",\n",
    "            penalty=\"elasticnet\",\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=20000,\n",
    "            random_state=SEED,\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        \"clf__C\": C_GRID,\n",
    "        \"clf__l1_ratio\": L1_RATIO_GRID,\n",
    "    }\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"roc_auc\",      # or \"average_precision\"\n",
    "        cv=INNER_SPLITS,        # simple CV inside TRAIN only\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "    gs.fit(X_tr, y_tr)\n",
    "\n",
    "    # Choose threshold on VALIDATION set (not test)\n",
    "    y_prob_val = gs.predict_proba(X_val)[:, 1]\n",
    "    best_threshold = choose_threshold_max_balanced_accuracy(y_val, y_prob_val)\n",
    "\n",
    "    print(f\"\\nBest params (from TRAIN CV): C={gs.best_params_['clf__C']}, l1_ratio={gs.best_params_['clf__l1_ratio']}\")\n",
    "    print(f\"Chosen threshold on VAL (max balanced accuracy): {best_threshold:.6f}\")\n",
    "\n",
    "    # Evaluate once on TEST set\n",
    "    y_prob_te = gs.predict_proba(X_te)[:, 1]\n",
    "    m_te = metrics_at_threshold(y_te, y_prob_te, best_threshold)\n",
    "\n",
    "    print(\"\\n=== TEST metrics (one-shot holdout) ===\")\n",
    "    print(f\"AUROC:             {m_te.auc:.3f}\")\n",
    "    print(f\"AUPRC:             {m_te.auprc:.3f}\")\n",
    "    print(f\"Balanced Accuracy: {m_te.balanced_accuracy:.3f}\")\n",
    "    print(f\"Sensitivity:       {m_te.sensitivity:.3f}\")\n",
    "    print(f\"Specificity:       {m_te.specificity:.3f}\")\n",
    "    print(f\"MCC:               {m_te.mcc:.3f}\")\n",
    "    print(f\"Brier:             {m_te.brier:.3f}\")\n",
    "\n",
    "    # PR + calibration summaries on TEST\n",
    "    prevalence_te = float(np.mean(y_te))\n",
    "    auprc_te = float(average_precision_score(y_te, y_prob_te))\n",
    "    ece_te = expected_calibration_error(y_te, y_prob_te, bins=10)\n",
    "\n",
    "    print(f\"\\nClass prevalence in TEST (AD): {prevalence_te:.3f}\")\n",
    "    print(f\"No-skill AUPRC baseline (TEST): {prevalence_te:.3f}\")\n",
    "    print(f\"AUPRC lift over baseline (TEST): {auprc_te - prevalence_te:.3f}\")\n",
    "    print(f\"Expected calibration error (10-bin, TEST): {ece_te:.3f}\")\n",
    "\n",
    "    # Save TEST predictions (no OOF)\n",
    "    out = df[[SAMPLE_COL, LABEL_COL]].copy()\n",
    "    out[\"y_true\"] = y\n",
    "    out[\"split\"] = \"unused\"\n",
    "    out.loc[X_tr.index, \"split\"] = \"train\"\n",
    "    out.loc[X_val.index, \"split\"] = \"val\"\n",
    "    out.loc[X_te.index, \"split\"] = \"test\"\n",
    "\n",
    "    out[\"y_prob\"] = np.nan\n",
    "    out.loc[X_val.index, \"y_prob\"] = y_prob_val\n",
    "    out.loc[X_te.index, \"y_prob\"] = y_prob_te\n",
    "\n",
    "    out[\"y_pred_thresh0p5\"] = np.where(np.isfinite(out[\"y_prob\"]), (out[\"y_prob\"] >= 0.5).astype(int), np.nan)\n",
    "    out[\"y_pred_bestthr\"] = np.where(np.isfinite(out[\"y_prob\"]), (out[\"y_prob\"] >= best_threshold).astype(int), np.nan)\n",
    "\n",
    "    out.to_csv(OUTDIR / \"holdout_predictions.csv\", index=False)\n",
    "\n",
    "    Path(BESTPARAMS_OUT).write_text(json.dumps({\n",
    "        \"best_C\": float(gs.best_params_[\"clf__C\"]),\n",
    "        \"best_l1_ratio\": float(gs.best_params_[\"clf__l1_ratio\"]),\n",
    "        \"threshold_chosen_on_val\": float(best_threshold),\n",
    "        \"splits\": {\"train\": int(len(X_tr)), \"val\": int(len(X_val)), \"test\": int(len(X_te))},\n",
    "    }, indent=2))\n",
    "\n",
    "    print(f\"\\nSaved holdout predictions to: {(OUTDIR / 'holdout_predictions.csv').resolve()}\")\n",
    "    print(f\"Saved best params summary to:       {BESTPARAMS_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac9b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after merge: 200 | AD: 158 | Control: 42\n",
      "Genes requested: 36 | present: 36 | missing: 0\n",
      "\n",
      "Best params (from TRAIN CV): C=3.1622776601683795, l1_ratio=0.0\n",
      "Chosen threshold on VAL (max balanced accuracy): 0.057926\n",
      "\n",
      "=== TEST metrics (one-shot holdout) ===\n",
      "AUROC:             0.882\n",
      "AUPRC:             0.965\n",
      "Balanced Accuracy: 0.621\n",
      "Sensitivity:       0.957\n",
      "Specificity:       0.286\n",
      "MCC:               0.342\n",
      "Brier:             0.132\n",
      "\n",
      "Class prevalence in TEST (AD): 0.767\n",
      "No-skill AUPRC baseline (TEST): 0.767\n",
      "AUPRC lift over baseline (TEST): 0.199\n",
      "Expected calibration error (10-bin, TEST): 0.119\n",
      "\n",
      "Saved holdout predictions to: C:\\Users\\kashvichirag\\Box\\Dr. ZHANG\\alzheimer_meta\\new_meta_analysis\\results\\holdout_predictions.csv\n",
      "Saved best params summary to:       C:\\Users\\kashvichirag\\Box\\Dr. ZHANG\\alzheimer_meta\\new_meta_analysis\\results\\best_params_per_fold.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kashvichirag\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
