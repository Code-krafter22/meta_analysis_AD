# External Validation Analysis (GSE125583)

This document evaluates whether a gene signature derived from the discovery/meta-analysis cohort (GSE184942) generalizes to an independent external cohort (GSE125583).

## 1) Load expression data and metadata, then build a sample-by-gene matrix

The block below:
- loads the selected-gene expression matrix and sample metadata,
- identifies the gene-symbol column,
- collapses duplicate gene symbols by mean expression,
- transposes to the machine-learning format (`samples x genes`), and
- derives AD/Control labels from metadata.

It also performs guardrail checks so the notebook fails early with a clear message if required columns are missing.

```{python}
import pandas as pd
import numpy as np

PATH_EXPR = "data/GSE125583/DE_data/GSE125583_log2cpm_SELECTED_GENES.csv"
PATH_META = "data/GSE125583/DE_data/metadata_200samples.csv"

df = pd.read_csv(PATH_EXPR)
meta = pd.read_csv(PATH_META)

required_meta_cols = {"geo_accession", "diagnosis:ch1"}
missing_meta = required_meta_cols - set(meta.columns)
if missing_meta:
    raise ValueError(f"Missing required metadata columns: {sorted(missing_meta)}")

# Identify gene-symbol column
if "Gene" in df.columns:
    gene_col = "Gene"
else:
    gene_col = df.columns[0]

# All non-gene columns are sample columns
sample_cols = [c for c in df.columns if c != gene_col]

# Build gene x sample matrix and collapse duplicated symbols
expr = df[[gene_col] + sample_cols].copy()
expr = expr.dropna(subset=[gene_col])
expr[gene_col] = expr[gene_col].astype(str)
expr = expr.groupby(gene_col, as_index=True)[sample_cols].mean()

# Transpose to samples x genes
X = expr.T
X.index.name = "geo_accession"

# Build labels from metadata
meta["geo_accession"] = meta["geo_accession"].astype(str)
meta["diagnosis:ch1"] = meta["diagnosis:ch1"].astype(str)

def to_label(s):
    s = s.lower()
    if "control" in s:
        return "Control"
    if "alzheimer" in s or s == "ad":
        return "AD"
    return np.nan

meta["Condition"] = meta["diagnosis:ch1"].apply(to_label)

# Align metadata and expression to shared samples
shared = X.index.intersection(meta["geo_accession"])
if len(shared) == 0:
    raise ValueError("No overlapping samples between expression matrix and metadata.")

X = X.loc[shared]
y = meta.set_index("geo_accession").loc[shared, "Condition"].values

# Remove samples with unresolved labels
keep = ~pd.isna(y)
X = X.loc[keep]
y = y[keep]

print("X shape (samples x genes):", X.shape)
print("Class counts:", dict(pd.Series(y).value_counts()))
print("Example genes:", list(X.columns[:10]))
```

## 2) Apply fixed external signature weights and compute ROC AUC

This is the core *external validation* step: we do **not retrain** model coefficients on the external cohort. Instead, we apply previously learned per-gene weights and evaluate discrimination.

To make this robust, we validate the expected `gene` and `weight` columns in the external weights file and stop if there are no overlapping genes.

```{python}
from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score
import matplotlib.pyplot as plt

WEIGHTS_PATH = "data/GSE184942/meta-analysis-weights-per-gene.xlsx"

w = pd.read_excel(WEIGHTS_PATH).dropna()
required_weight_cols = {"gene", "weight"}
missing_weight = required_weight_cols - set(w.columns)
if missing_weight:
    raise ValueError(f"Missing required weight columns: {sorted(missing_weight)}")

w["gene"] = w["gene"].astype(str)

# Restrict to genes shared by external data and signature
common = sorted(set(X.columns).intersection(set(w["gene"])))
print("Common genes with weights:", len(common))
if len(common) == 0:
    raise ValueError("No shared genes found between external expression matrix and signature weights.")

w = w.set_index("gene").loc[common]
Xc = X[common]

# Signature score (linear weighted sum)
score = Xc.values @ w["weight"].values

# ROC AUC on external cohort
y_bin = (y == "AD").astype(int)
sig_auc = roc_auc_score(y_bin, score)
prevalence = y_bin.mean()
sig_ap_point = average_precision_score(y_bin, score)
print(f"No-skill AP baseline (prevalence): {prevalence:.3f}")
print("Signature ROC AUC:", sig_auc)
print(f"Signature AP (point estimate): {sig_ap_point:.3f}")

# ROC plot
fpr, tpr, _ = roc_curve(y_bin, score)
plt.figure(figsize=(5, 5))
plt.plot(fpr, tpr, linewidth=2)
plt.plot([0, 1], [0, 1], linestyle="--", linewidth=1)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(f"36-Gene Signature ROC (AUC={sig_auc:.3f})")
plt.tight_layout()
plt.savefig("Signature_ROC_600dpi.png", dpi=600, bbox_inches="tight")
plt.show()
```

## 3) Estimate uncertainty for signature performance with bootstrap confidence intervals

Point estimates alone can be unstable. This block computes 95% bootstrap confidence intervals for ROC AUC and Average Precision (AP).

```{python}
from sklearn.metrics import average_precision_score
from sklearn.calibration import calibration_curve
from scipy.special import expit

def bootstrap_ci_auc(y_true, y_score, n_boot=5000, seed=1, ci=0.95):
    rng = np.random.default_rng(seed)
    y_true = np.asarray(y_true)
    y_score = np.asarray(y_score)

    auc_point = roc_auc_score(y_true, y_score)
    boot = []
    n = len(y_true)

    for _ in range(n_boot):
        idx = rng.integers(0, n, size=n)
        yt = y_true[idx]
        ys = y_score[idx]
        if len(np.unique(yt)) < 2:
            continue
        boot.append(roc_auc_score(yt, ys))

    boot = np.array(boot)
    alpha = (1 - ci) / 2
    return auc_point, (np.quantile(boot, alpha), np.quantile(boot, 1 - alpha)), boot


def bootstrap_ci_ap(y_true, y_score, n_boot=5000, seed=1, ci=0.95):
    rng = np.random.default_rng(seed)
    y_true = np.asarray(y_true)
    y_score = np.asarray(y_score)

    ap_point = average_precision_score(y_true, y_score)
    boot = []
    n = len(y_true)

    for _ in range(n_boot):
        idx = rng.integers(0, n, size=n)
        yt = y_true[idx]
        ys = y_score[idx]
        if len(np.unique(yt)) < 2:
            continue
        boot.append(average_precision_score(yt, ys))

    boot = np.array(boot)
    alpha = (1 - ci) / 2
    return ap_point, (np.quantile(boot, alpha), np.quantile(boot, 1 - alpha)), boot


sig_auc, sig_ci, _ = bootstrap_ci_auc(y_bin, score, n_boot=5000, seed=1)
sig_ap, sig_ap_ci, _ = bootstrap_ci_ap(y_bin, score, n_boot=5000, seed=1)

print(f"Signature ROC AUC = {sig_auc:.3f} (95% CI {sig_ci[0]:.3f}–{sig_ci[1]:.3f})")
print(f"Signature AP = {sig_ap:.3f} (95% CI {sig_ap_ci[0]:.3f}–{sig_ap_ci[1]:.3f})")

# Calibration check for a probability-like transformed signature score
score_prob = expit((score - score.mean()) / score.std())
fraction_pos, mean_pred = calibration_curve(y_bin, score_prob, n_bins=10)

plt.figure(figsize=(5, 5))
plt.plot(mean_pred, fraction_pos, marker="o", label="Signature")
plt.plot([0, 1], [0, 1], linestyle="--", label="Perfect calibration")
plt.xlabel("Mean predicted probability")
plt.ylabel("Fraction positive")
plt.title("Calibration Plot — Fixed Signature")
plt.legend()
plt.tight_layout()
plt.savefig("Signature_Calibration_600dpi.png", dpi=600, bbox_inches="tight")
plt.show()
```

## 4) Fit a baseline nested-CV elastic-net model on the external cohort (secondary analysis)

This section is a **secondary benchmark**, not strict external validation, because it retrains in the external data. It can be useful for context but should be reported separately from the fixed-signature result.

```{python}
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    roc_auc_score,
    balanced_accuracy_score,
    precision_recall_curve,
    auc,
    confusion_matrix,
    brier_score_loss,
)
import warnings

X_np = X.values
y_np = (y == "AD").astype(int)

print(f"Class distribution — AD: {y_np.sum()}, Control: {(1-y_np).sum()}, Prevalence: {y_np.mean():.3f}")

outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)

pipe = Pipeline([
    ("scaler", StandardScaler()),
    (
        "clf",
        LogisticRegression(
            solver="saga",
            penalty="elasticnet",
            max_iter=20000,
            class_weight="balanced",
            random_state=1,
        ),
    ),
])

param_grid = {
    "clf__C": [0.01, 0.1, 1, 10, 100],
    "clf__l1_ratio": [0.0, 0.25, 0.5, 0.75, 1.0],
}

y_proba = np.zeros(len(y_np))
y_pred = np.zeros(len(y_np), dtype=int)
best_params_each_fold = []

for train_idx, test_idx in outer.split(X_np, y_np):
    Xtr, Xte = X_np[train_idx], X_np[test_idx]
    ytr, yte = y_np[train_idx], y_np[test_idx]

    gs = GridSearchCV(pipe, param_grid=param_grid, scoring="roc_auc", cv=inner, n_jobs=-1)
    gs.fit(Xtr, ytr)
    best_params_each_fold.append(gs.best_params_)

    proba = gs.predict_proba(Xte)[:, 1]
    pred = (proba >= 0.5).astype(int)

    y_proba[test_idx] = proba
    y_pred[test_idx] = pred

rocAUC = roc_auc_score(y_np, y_proba)
balAcc = balanced_accuracy_score(y_np, y_pred)
prec, rec, _ = precision_recall_curve(y_np, y_proba)
prAUC = auc(rec, prec)
cm = confusion_matrix(y_np, y_pred)

print("Nested-CV ROC AUC:", rocAUC)
print("Nested-CV PR AUC:", prAUC)
print("Nested-CV Balanced Accuracy:", balAcc)
print("Confusion matrix [[TN FP],[FN TP]]:\n", cm)
print("Best params per fold:\n", best_params_each_fold)

# Additional context for class imbalance and model stability
brier = brier_score_loss(y_np, y_proba)
print(f"Nested-CV Brier Score: {brier:.3f} (lower is better; no-skill baseline = {prevalence * (1 - prevalence):.3f})")

ncv_auc, ncv_ci, _ = bootstrap_ci_auc(y_np, y_proba, n_boot=5000, seed=1)
print(f"Nested-CV ROC AUC = {ncv_auc:.3f} (95% CI {ncv_ci[0]:.3f}–{ncv_ci[1]:.3f})")

l1_ratios = [p["clf__l1_ratio"] for p in best_params_each_fold]
C_vals = [p["clf__C"] for p in best_params_each_fold]
if len(set(l1_ratios)) > 1 or len(set(C_vals)) > 1:
    warnings.warn(
        "Regularization parameters vary across outer folds "
        f"(C={C_vals}, l1_ratio={l1_ratios}). "
        "Elastic-net solution may be unstable; interpret nested-CV AUC cautiously.",
        UserWarning,
        stacklevel=2,
    )
```

## 5) Plot nested-CV ROC and PR curves

This block visualizes the nested-CV benchmark outputs.

```{python}
from sklearn.metrics import roc_curve

# Nested-CV ROC
fpr_cv, tpr_cv, _ = roc_curve(y_np, y_proba)
plt.figure(figsize=(5, 5))
plt.plot(fpr_cv, tpr_cv, linewidth=2)
plt.plot([0, 1], [0, 1], linestyle="--", linewidth=1)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(f"Nested-CV ROC (AUC={rocAUC:.3f})")
plt.tight_layout()
plt.savefig("NestedCV_ROC_600dpi.png", dpi=600, bbox_inches="tight")
plt.show()

# Nested-CV PR
ap = prAUC  # already computed in Section 4
plt.figure(figsize=(5, 5))
plt.plot(rec, prec, linewidth=2)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title(f"Nested-CV PR Curve (AP={ap:.3f})")
plt.tight_layout()
plt.savefig("NestedCV_PR_600dpi.png", dpi=600, bbox_inches="tight")
plt.show()
```

## 6) Optional threshold analysis for operating-point selection

AUC summarizes ranking performance; deployment often needs a decision threshold. This block finds:
- threshold maximizing balanced accuracy,
- threshold achieving a minimum specificity target.

```{python}
from sklearn.metrics import confusion_matrix

# (A) Threshold that maximizes balanced accuracy
thr_grid = np.linspace(0, 1, 1001)
best_thr_ba, best_ba = None, -1

for thr in thr_grid:
    pred = (y_proba >= thr).astype(int)
    ba = balanced_accuracy_score(y_np, pred)
    if ba > best_ba:
        best_ba, best_thr_ba = ba, thr

print("Best threshold (max balanced acc):", best_thr_ba)
print("Best balanced acc:", best_ba)
print("CM at best BA threshold:\n", confusion_matrix(y_np, (y_proba >= best_thr_ba).astype(int)))

# (B) Max sensitivity subject to specificity >= target

def spec_sens(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan
    sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan
    return spec, sens

target_spec = 0.80
best_thr_spec, best_sens = None, -1

for thr in thr_grid:
    pred = (y_proba >= thr).astype(int)
    spec, sens = spec_sens(y_np, pred)
    if spec >= target_spec and sens > best_sens:
        best_sens, best_thr_spec = sens, thr

print("Threshold with specificity >= 0.80 and max sensitivity:", best_thr_spec)
if best_thr_spec is not None:
    pred = (y_proba >= best_thr_spec).astype(int)
    print("Spec, Sens:", spec_sens(y_np, pred))
    print("CM:\n", confusion_matrix(y_np, pred))
else:
    print("No threshold met the specificity target.")

print(
    "\nNOTE: Thresholds selected on out-of-fold predictions are still optimistically biased. "
    "These operating points should be validated on a truly held-out dataset before clinical use."
)
```

## 7) Summarize primary vs secondary analyses in a compact table

This table helps reporting by clearly separating the fixed-signature external validation (primary) from nested-CV retraining (secondary).

```{python}
results = pd.DataFrame({
    "Analysis": [
        "Fixed Signature (External Validation)",
        "Nested-CV Elastic-Net (Secondary Benchmark)",
    ],
    "ROC AUC": [f"{sig_auc:.3f}", f"{ncv_auc:.3f}"],
    "95% CI": [f"{sig_ci[0]:.3f}–{sig_ci[1]:.3f}", f"{ncv_ci[0]:.3f}–{ncv_ci[1]:.3f}"],
    "AP": [f"{sig_ap:.3f}", f"{prAUC:.3f}"],
    "AP baseline (prevalence)": [f"{prevalence:.3f}", f"{prevalence:.3f}"],
    "Note": [
        "No retraining — primary result",
        "Retrained on external data — secondary",
    ],
})

print(results.to_string(index=False))
results.to_csv("validation_summary_table.csv", index=False)
```

## Recommendations

- Treat **Section 2** (fixed-weight signature applied once to external data) as the primary external-validation result.
- Treat **Sections 4–6** (nested-CV retraining + threshold tuning) as secondary benchmarking/optimization analyses.
- If clinical probability interpretation is needed, add calibration (e.g., isotonic/Platt) within nested CV and report calibration curves/Brier score.
